{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=My First Spark App, master=yarn) created by __init__ at /tmp/ipykernel_28869/3844447949.py:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28869/3844447949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# conf = pyspark.SparkConf().setAppName(\"My First Spark RDD APP\").setMaster(\"local\")  # “yarn”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sc = pyspark.SparkContext(conf=conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yarn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"My First Spark App\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark-current/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/spark-current/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=My First Spark App, master=yarn) created by __init__ at /tmp/ipykernel_28869/3844447949.py:3 "
     ]
    }
   ],
   "source": [
    "# conf = pyspark.SparkConf().setAppName(\"My First Spark RDD APP\").setMaster(\"local\")  # “yarn”\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "sc = pyspark.SparkContext(\"yarn\", \"My First Spark App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 4.],\n",
      "             [2., 5.],\n",
      "             [3., 6.]])\n",
      "3 X 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "print(dm2)\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airdelay_small.csv  data_info.csv  dummy_info.pkl  dummy_info_small.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls airdelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"/lifeng/student/liutuozhen/airdelay_small.csv\") # load from hdfs\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5548755"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lineLengths = textFile.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[23] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534280797"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession是Spark2.x后引入的概念。在2.x之前，对于不同的功能，需要使用不同的Context：\n",
    "\n",
    "\n",
    "* 创建和操作RDD时，使用SparkContext\n",
    "\n",
    "* 使用Streaming时，使用StreamingContext\n",
    "\n",
    "* 使用SQL时，使用sqlContext\n",
    "\n",
    "* 使用Hive时，使用HiveContext\n",
    "\n",
    "\n",
    "在2.x中，为了统一上述的Context，引入SparkSession，实质上是SQLContext、HiveContext、SparkContext的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No such property [datePattern] in org.apache.log4j.RollingFileAppender.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/01 19:31:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/01 19:31:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/01 19:31:38 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/12/01 19:31:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark\").master('yarn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext # for RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://emr-header-1.cluster-46968:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f36dfcd6c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix, BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = RowMatrix(sc.parallelize([[1,1,3],[4,2,6],[6,4,9]]))\n",
    "y = RowMatrix(sc.parallelize([1,3,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTx = x.computeGramianMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 3, [53.0, 33.0, 81.0, 33.0, 21.0, 51.0, 81.0, 51.0, 126.0], 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xTx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = RowMatrix(sc.parallelize([[1,1,3,1],[4,2,6,3],[6,4,9,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyTxy = xy.computeGramianMatrix().toArray().reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTy = xyTxy[:-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49., 31., 75.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTx = xyTxy[:-1, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53.,  33.,  81.],\n",
       "       [ 33.,  21.,  51.],\n",
       "       [ 81.,  51., 126.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = np.linalg.solve(xTx, xTy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5       ,  1.5       , -0.33333333])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyTxy = xy.computeGramianMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = xyTxy.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53.,  33.,  81.,  49.],\n",
       "       [ 33.,  21.,  51.,  31.],\n",
       "       [ 81.,  51., 126.,  75.],\n",
       "       [ 49.,  31.,  75.,  46.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DenseMatrix' object has no attribute 'multiply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27358/277848274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxyTxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDenseMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DenseMatrix' object has no attribute 'multiply'"
     ]
    }
   ],
   "source": [
    "xyTxy.multiply(DenseMatrix(4, 1, [0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 53.,  33.,  81.,  49.,  33.,  21.,  51.,  31.,  81.,  51., 126.,\n",
       "        75.,  49.,  31.,  75.,  46.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyTxy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/lifeng/student/liutuozhen/streaming/aggregate/table.csv MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = sc.textFile(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = spark.read.text(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.filter(textFile.value.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'contains(value, Spark)'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.value.contains(\"Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "    words = s.split(\",\")\n",
    "    return len([w for w in words if w != \"北京\"])\n",
    "\n",
    "textFile.map(myFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "global counter\n",
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    print(x)\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_counter(x):\n",
    "    global counter\n",
    "    print(counter)\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark_df from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'北京,女,7'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "cols = parts.map(lambda p: Row(area=p[0], num=int(p[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[area: string, num: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(cols)\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|area|num|\n",
      "+----+---+\n",
      "|北京|  7|\n",
      "|江苏|  8|\n",
      "|上海| 18|\n",
      "|北京| 45|\n",
      "|江苏| 77|\n",
      "|江苏| 24|\n",
      "|江苏| 77|\n",
      "|江苏| 31|\n",
      "|上海| 71|\n",
      "|江苏| 57|\n",
      "|上海| 72|\n",
      "|浙江| 66|\n",
      "|北京| 19|\n",
      "|江苏|  9|\n",
      "|浙江| 72|\n",
      "|北京| 16|\n",
      "|北京| 61|\n",
      "|北京| 21|\n",
      "|浙江| 29|\n",
      "|上海| 34|\n",
      "+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|area|num|\n",
      "+----+---+\n",
      "|江苏| 77|\n",
      "|江苏| 77|\n",
      "|上海| 71|\n",
      "|上海| 72|\n",
      "|浙江| 72|\n",
      "|上海| 75|\n",
      "|北京| 76|\n",
      "|江苏| 72|\n",
      "|上海| 74|\n",
      "|江苏| 72|\n",
      "|浙江| 78|\n",
      "|上海| 76|\n",
      "|浙江| 74|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.createOrReplaceTempView(\"tempDF\") # 建立临时表tempDF\n",
    "filters = spark.sql(\"SELECT * FROM tempDF WHERE num>70\")\n",
    "filters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无分布式 to Master Node\n",
    "pdf = filters.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"area\":\"江苏\",\"num\":77}',\n",
       " '{\"area\":\"江苏\",\"num\":77}',\n",
       " '{\"area\":\"上海\",\"num\":71}',\n",
       " '{\"area\":\"上海\",\"num\":72}',\n",
       " '{\"area\":\"浙江\",\"num\":72}',\n",
       " '{\"area\":\"上海\",\"num\":75}',\n",
       " '{\"area\":\"北京\",\"num\":76}',\n",
       " '{\"area\":\"江苏\",\"num\":72}',\n",
       " '{\"area\":\"上海\",\"num\":74}',\n",
       " '{\"area\":\"江苏\",\"num\":72}',\n",
       " '{\"area\":\"浙江\",\"num\":78}',\n",
       " '{\"area\":\"上海\",\"num\":76}',\n",
       " '{\"area\":\"浙江\",\"num\":74}']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json = filters.toJSON()\n",
    "df_json.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark_df from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "| _c0|_c1|_c2|\n",
      "+----+---+---+\n",
      "|北京| 女|  7|\n",
      "|江苏| 男|  8|\n",
      "|上海| 男| 18|\n",
      "|北京| 女| 45|\n",
      "|江苏| 男| 77|\n",
      "|江苏| 男| 24|\n",
      "|江苏| 男| 77|\n",
      "|江苏| 女| 31|\n",
      "|上海| 女| 71|\n",
      "|江苏| 男| 57|\n",
      "|上海| 女| 72|\n",
      "|浙江| 男| 66|\n",
      "|北京| 男| 19|\n",
      "|江苏| 男|  9|\n",
      "|浙江| 男| 72|\n",
      "|北京| 男| 16|\n",
      "|北京| 女| 61|\n",
      "|北京| 男| 21|\n",
      "|浙江| 女| 29|\n",
      "|上海| 男| 34|\n",
      "+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# local mode: local path / yarn mode: hdfs path\n",
    "sdf = spark.read.csv(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema for csv header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# The schema is encoded in a string.\n",
    "\n",
    "# Create a schema\n",
    "schemaString = [\"area\", \"gender\", \"num\"]\n",
    "StringType = [StringType(), StringType(), IntegerType()]\n",
    "fields = [StructField(field_name, string_type, True) for field_name, string_type in zip(schemaString, StringType)] # True: 是否允许空值\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(area,StringType,true),StructField(gender,StringType,true),StructField(num,IntegerType,true)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---+\n",
      "|area|gender|num|\n",
      "+----+------+---+\n",
      "|北京|    女|  7|\n",
      "|江苏|    男|  8|\n",
      "|上海|    男| 18|\n",
      "|北京|    女| 45|\n",
      "|江苏|    男| 77|\n",
      "|江苏|    男| 24|\n",
      "|江苏|    男| 77|\n",
      "|江苏|    女| 31|\n",
      "|上海|    女| 71|\n",
      "|江苏|    男| 57|\n",
      "|上海|    女| 72|\n",
      "|浙江|    男| 66|\n",
      "|北京|    男| 19|\n",
      "|江苏|    男|  9|\n",
      "|浙江|    男| 72|\n",
      "|北京|    男| 16|\n",
      "|北京|    女| 61|\n",
      "|北京|    男| 21|\n",
      "|浙江|    女| 29|\n",
      "|上海|    男| 34|\n",
      "+----+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_with_schema = spark.read.csv(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\", schema)\n",
    "sdf_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DF to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[95] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_with_schema.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DF to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_with_schema.write.mode(\"overwrite\").csv(\"/lifeng/student/liutuozhen/sdf_save/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r-----   2 student hadoop          0 2021-11-10 19:41 /lifeng/student/liutuozhen/sdf_save/_SUCCESS\r\n",
      "-rw-r-----   2 student hadoop       1390 2021-11-10 19:41 /lifeng/student/liutuozhen/sdf_save/part-00000-bb23d8ed-5d01-4a90-886e-041bd0535172-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /lifeng/student/liutuozhen/sdf_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file and infer the schema from the header¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    " ## Load a local file becasue we are using spark's local mode\n",
    "air0 = spark.read.options(header='true', inferSchema='true').csv(\"/lifeng/student/liutuozhen/airdelay_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: string, CRSDepTime: int, ArrTime: string, CRSArrTime: int, UniqueCarrier: string, FlightNum: int, TailNum: string, ActualElapsedTime: string, CRSElapsedTime: string, AirTime: string, ArrDelay: string, DepDelay: string, Origin: string, Dest: string, Distance: string, TaxiIn: string, TaxiOut: string, Cancelled: int, CancellationCode: string, Diverted: int, CarrierDelay: string, WeatherDelay: string, NASDelay: string, SecurityDelay: string, LateAircraftDelay: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air0 # the schema is not correct for some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify the correct schema by hand\n",
    "from pyspark.sql.types import *\n",
    "schema_sdf = StructType([\n",
    "        StructField('Year', IntegerType(), True),\n",
    "        StructField('Month', IntegerType(), True),\n",
    "        StructField('DayofMonth', IntegerType(), True),\n",
    "        StructField('DayOfWeek', IntegerType(), True),\n",
    "        StructField('DepTime', DoubleType(), True),\n",
    "        StructField('CRSDepTime', DoubleType(), True),\n",
    "        StructField('ArrTime', DoubleType(), True),\n",
    "        StructField('CRSArrTime', DoubleType(), True),\n",
    "        StructField('UniqueCarrier', StringType(), True),\n",
    "        StructField('FlightNum', StringType(), True),\n",
    "        StructField('TailNum', StringType(), True),\n",
    "        StructField('ActualElapsedTime', DoubleType(), True),\n",
    "        StructField('CRSElapsedTime',  DoubleType(), True),\n",
    "        StructField('AirTime',  DoubleType(), True),\n",
    "        StructField('ArrDelay',  DoubleType(), True),\n",
    "        StructField('DepDelay',  DoubleType(), True),\n",
    "        StructField('Origin', StringType(), True),\n",
    "        StructField('Dest',  StringType(), True),\n",
    "        StructField('Distance',  DoubleType(), True),\n",
    "        StructField('TaxiIn',  DoubleType(), True),\n",
    "        StructField('TaxiOut',  DoubleType(), True),\n",
    "        StructField('Cancelled',  IntegerType(), True),\n",
    "        StructField('CancellationCode',  StringType(), True),\n",
    "        StructField('Diverted',  IntegerType(), True),\n",
    "        StructField('CarrierDelay', DoubleType(), True),\n",
    "        StructField('WeatherDelay',  DoubleType(), True),\n",
    "        StructField('NASDelay',  DoubleType(), True),\n",
    "        StructField('SecurityDelay',  DoubleType(), True),\n",
    "        StructField('LateAircraftDelay',  DoubleType(), True)\n",
    "    ])\n",
    "air = spark.read.options(header='true').schema(schema_sdf).csv(\"/lifeng/student/liutuozhen/airdelay_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: double, CRSDepTime: double, ArrTime: double, CRSArrTime: double, UniqueCarrier: string, FlightNum: string, TailNum: string, ActualElapsedTime: double, CRSElapsedTime: double, AirTime: double, ArrDelay: double, DepDelay: double, Origin: string, Dest: string, Distance: double, TaxiIn: double, TaxiOut: double, Cancelled: int, CancellationCode: string, Diverted: int, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+-------+------------------+------------------+------------------+-----------------+-----------------+-------+-------+-----------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+--------------------+-----------------+\n",
      "|summary|              Year|             Month|        DayofMonth|         DayOfWeek|           DepTime|       CRSDepTime|           ArrTime|        CRSArrTime|UniqueCarrier|         FlightNum|TailNum| ActualElapsedTime|    CRSElapsedTime|           AirTime|         ArrDelay|         DepDelay| Origin|   Dest|         Distance|            TaxiIn|           TaxiOut|Cancelled|CancellationCode|Diverted|      CarrierDelay|      WeatherDelay|         NASDelay|       SecurityDelay|LateAircraftDelay|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+-------+------------------+------------------+------------------+-----------------+-----------------+-------+-------+-----------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+--------------------+-----------------+\n",
      "|  count|           1524481|           1524481|           1524481|           1524481|           1524481|          1524481|           1524481|           1524481|      1524481|           1524481|1524481|           1524481|           1524481|           1524481|          1524481|          1524481|1524481|1524481|          1524481|           1524481|           1524481|  1524481|               0| 1524481|           1524481|           1524481|          1524481|             1524481|          1524481|\n",
      "|   mean|2005.2200309482375| 6.819439533847913|15.739877374660622|3.9425345412635515|1343.1007890554229|1335.312045870037|1490.5321424143692|1499.0464518744411|         null|2100.4755290489024|    0.0|124.89908893584112|126.23595243233599|102.60629092786331|7.728503667805634|9.065091660702889|   null|   null|723.8429826281863|7.4043402311999955|16.027061012895537|      0.0|            null|     0.0|3.1917360728011697|0.6896806191746568|3.555705187535955|0.025394216129948487|4.100354809276075|\n",
      "| stddev|1.3298684774182634|3.3772885531435226| 8.788305183327518|1.9915258421756026|  475.476667807491|462.8261301202433|499.16257581932297| 478.3461722588969|         null|1937.4578676903252|    0.0| 70.91261333146292|  70.1171682758882| 78.48819536331372|35.41450193208645| 32.1692195863043|   null|   null|571.7621650788843|35.986895606761344|12.080643220148529|      0.0|            null|     0.0|18.414985451675538| 8.708781309482003|15.28241309586754|   1.140672940233062|18.70428806839677|\n",
      "|    min|              2003|                 1|                 1|                 1|               1.0|              3.0|               1.0|               0.0|           9E|                 1|      0|            -681.0|              16.0|           -1461.0|           -735.0|          -1197.0|    ABE|    ABE|             27.0|               0.0|               0.0|        0|            null|       0|               0.0|               0.0|            -13.0|                 0.0|              0.0|\n",
      "|    max|              2007|                12|                31|                 7|            2644.0|           2359.0|            2742.0|            2359.0|           YV|               999| n816ca|            1766.0|             660.0|            1936.0|           1779.0|           1752.0|    YUM|    YUM|           4962.0|            1470.0|            1439.0|        0|            null|       0|            1665.0|             910.0|           1010.0|               382.0|           1060.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+-------+------------------+------------------+------------------+-----------------+-----------------+-------+-------+-----------------+------------------+------------------+---------+----------------+--------+------------------+------------------+-----------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "air.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==================================================>     (29 + 3) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          ArrDelay|\n",
      "+-------+------------------+\n",
      "|  count|           5432958|\n",
      "|   mean|  6.97897995898367|\n",
      "| stddev|30.191156753519472|\n",
      "|    min|           -1238.0|\n",
      "|    max|            1779.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "air.describe(['ArrDelay']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: double (nullable = true)\n",
      " |-- CRSDepTime: double (nullable = true)\n",
      " |-- ArrTime: double (nullable = true)\n",
      " |-- CRSArrTime: double (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- CRSElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- TaxiIn: double (nullable = true)\n",
      " |-- TaxiOut: double (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|UniqueCarrier|(ArrDelay > 0)|\n",
      "+-------------+--------------+\n",
      "|           XE|          true|\n",
      "|           CO|          true|\n",
      "|           AA|          true|\n",
      "|           WN|         false|\n",
      "|           CO|          true|\n",
      "|           AA|          true|\n",
      "|           DL|         false|\n",
      "|           AA|          true|\n",
      "|           US|          true|\n",
      "|           AA|          true|\n",
      "|           AS|         false|\n",
      "|           UA|          true|\n",
      "|           TW|         false|\n",
      "|           NW|          true|\n",
      "|           NW|          true|\n",
      "|           AA|          true|\n",
      "|           DH|          true|\n",
      "|           WN|         false|\n",
      "|           AA|         false|\n",
      "|           CO|          true|\n",
      "+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air.select(air['UniqueCarrier'], air['ArrDelay']>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 19:44:04 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "[Stage 3:================================================>      (175 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|UniqueCarrier| count|\n",
      "+-------------+------+\n",
      "|           DL|765388|\n",
      "|           WN|703368|\n",
      "|           AA|684522|\n",
      "|           US|649056|\n",
      "|           UA|611957|\n",
      "|           NW|473820|\n",
      "|           CO|373858|\n",
      "|           TW|179081|\n",
      "|           HP|173509|\n",
      "|           MQ|164790|\n",
      "|           AS|129863|\n",
      "|           OO|120223|\n",
      "|           XE| 94311|\n",
      "|           EV| 67148|\n",
      "|           OH| 60630|\n",
      "|           FL| 47540|\n",
      "|           EA| 43723|\n",
      "|           PI| 41489|\n",
      "|           DH| 32900|\n",
      "|           B6| 29111|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Group and sort\n",
    "aircount=air.groupBy(\"UniqueCarrier\").count()\n",
    "aircount.sort(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1524482"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2006|    7|         6|        4| 2055.0|    2055.0| 2150.0|    2148.0|           XE|     2619| N11526|             55.0|          53.0|   25.0|     2.0|     0.0|   IAH| LCH|   127.0|   8.0|   22.0|        0|            null|       0|         0.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|2007|   10|        26|        5|  654.0|     700.0| 1507.0|    1515.0|           AS|      802| N648AS|            313.0|         315.0|  293.0|    -8.0|    -6.0|   PDX| BOS|  2537.0|   9.0|   11.0|        0|            null|       0|         0.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|2004|   10|        18|        1|  939.0|     940.0| 1151.0|    1128.0|           NW|      679| N317NB|            192.0|         168.0|  149.0|    23.0|    -1.0|   MSP| SLC|   991.0|  10.0|   33.0|        0|            null|       0|         0.0|         0.0|    23.0|          0.0|              0.0|\n",
      "|2007|    5|        17|        4| 1944.0|    1830.0| 2034.0|    1930.0|           AA|     1011| N3BDAA|             50.0|          60.0|   35.0|    64.0|    74.0|   MIA| MCO|   193.0|   4.0|   11.0|        0|            null|       0|        64.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2003|    7|        31|        4| 1738.0|    1710.0| 1819.0|    1750.0|           DH|     7981| N309UE|             41.0|          40.0|   25.0|    29.0|    28.0|   IAD| CHO|    77.0|   3.0|   12.0|        0|            null|       0|        29.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "|null| null|      null|     null|   null|      null|   null|      null|         null|     null|   null|             null|          null|   null|    null|    null|  null|null|    null|  null|   null|     null|            null|    null|        null|        null|    null|         null|             null|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air.na.replace(\"NA\", \"unknown\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max(DayOfWeek)|\n",
      "+--------------+\n",
      "|             7|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "air.groupBy().max('DayOfWeek').show() # mus apply to a numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2007|    5|        17|        4| 1944.0|    1830.0| 2034.0|    1930.0|           AA|     1011| N3BDAA|             50.0|          60.0|   35.0|    64.0|    74.0|   MIA| MCO|   193.0|   4.0|   11.0|        0|            null|       0|        64.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2004|    4|        16|        5| 1550.0|    1525.0| 1750.0|    1638.0|           XE|     2602| N15948|            120.0|          73.0|   35.0|    72.0|    25.0|   EWR| BWI|   169.0|   5.0|   80.0|        0|            null|       0|         0.0|         0.0|    51.0|          0.0|             21.0|\n",
      "|2005|    7|         1|        5| 1931.0|    1723.0| 2235.0|    2017.0|           FL|      831| N978AT|            124.0|         114.0|   86.0|   138.0|   128.0|   MDW| ATL|   590.0|   9.0|   29.0|        0|            null|       0|         0.0|         0.0|    10.0|          0.0|            128.0|\n",
      "|2005|   11|        22|        2| 1849.0|    1705.0| 2034.0|    1815.0|           B6|       55| N585JB|            105.0|          70.0|   56.0|   139.0|   104.0|   BTV| JFK|   267.0|   4.0|   45.0|        0|            null|       0|         0.0|         0.0|    35.0|          0.0|            104.0|\n",
      "|2007|   10|        21|        7| 1725.0|    1625.0| 1837.0|    1730.0|           WN|      143| N389SW|             72.0|          65.0|   42.0|    67.0|    60.0|   LAS| LAX|   236.0|  12.0|   18.0|        0|            null|       0|         0.0|         0.0|     7.0|          0.0|             60.0|\n",
      "|2005|    4|        15|        5| 1519.0|    1310.0| 1751.0|    1545.0|           AS|      631| N947AS|            152.0|         155.0|  136.0|   126.0|   129.0|   LAS| SEA|   866.0|   5.0|   11.0|        0|            null|       0|         4.0|         0.0|     0.0|          0.0|            122.0|\n",
      "|2006|    1|        24|        2| 1230.0|    1115.0| 1343.0|    1228.0|           OH|     5050| N784CA|             73.0|          73.0|   50.0|    75.0|    75.0|   JFK| BOS|   187.0|   5.0|   18.0|        0|            null|       0|        75.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2007|    6|        16|        6| 1525.0|    1155.0| 1636.0|    1255.0|           XE|     2317| N14933|             71.0|          60.0|   44.0|   221.0|   210.0|   LFT| IAH|   201.0|  10.0|   17.0|        0|            null|       0|         0.0|         0.0|   104.0|          0.0|            117.0|\n",
      "|2006|   12|         1|        5| 1016.0|     720.0| 1226.0|     925.0|           MQ|     3484| N902BC|            190.0|         185.0|  167.0|   181.0|   176.0|   CHS| DFW|   987.0|  11.0|   12.0|        0|            null|       0|       176.0|         0.0|     5.0|          0.0|              0.0|\n",
      "|2007|    8|        25|        6| 1544.0|    1430.0| 2123.0|    2015.0|           AA|     2073| N612AA|            279.0|         285.0|  256.0|    68.0|    74.0|   ORD| SJU|  2072.0|   4.0|   19.0|        0|            null|       0|        68.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2006|    7|        20|        4| 1102.0|     834.0| 1445.0|    1225.0|           AA|     1652| N441AA|            163.0|         171.0|  144.0|   140.0|   148.0|   ORD| RSW|  1120.0|   2.0|   17.0|        0|            null|       0|         0.0|       140.0|     0.0|          0.0|              0.0|\n",
      "|2006|    7|        23|        7| 2110.0|    1920.0| 2228.0|    2035.0|           WN|     2347|   N433|             78.0|          75.0|   57.0|   113.0|   110.0|   SNA| OAK|   371.0|   6.0|   15.0|        0|            null|       0|         0.0|         0.0|     3.0|          4.0|            106.0|\n",
      "|2007|    1|        14|        7| 1110.0|     850.0| 1216.0|    1010.0|           9E|     5968| 85889E|             66.0|          80.0|   44.0|   126.0|   140.0|   MEM| STL|   256.0|   5.0|   17.0|        0|            null|       0|       126.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2004|    1|        11|        7| 1530.0|    1255.0| 1815.0|    1545.0|           WN|     1405|   N630|            105.0|         110.0|   94.0|   150.0|   155.0|   OAK| PHX|   646.0|   3.0|    8.0|        0|            null|       0|       150.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2007|   10|        26|        5| 1603.0|    1448.0| 1900.0|    1752.0|           UA|      270| N535UA|            117.0|         124.0|  105.0|    68.0|    75.0|   SNA| DEN|   846.0|   5.0|    7.0|        0|            null|       0|         0.0|         0.0|     0.0|          0.0|             68.0|\n",
      "|2005|   10|        31|        1|  854.0|     858.0| 1136.0|    1000.0|           MQ|     3384| N852AE|            162.0|          62.0|   41.0|    96.0|    -4.0|   SJT| DFW|   228.0| 109.0|   12.0|        0|            null|       0|         0.0|         0.0|    96.0|          0.0|              0.0|\n",
      "|2006|    1|         6|        5| 2200.0|    2030.0| 2320.0|    2214.0|           OH|     5830| N408CA|            140.0|         164.0|  119.0|    66.0|    90.0|   LGA| BHM|   866.0|   5.0|   16.0|        0|            null|       0|         0.0|         0.0|     0.0|          0.0|             66.0|\n",
      "|2005|    6|         1|        3| 1729.0|    1610.0| 1847.0|    1740.0|           AA|     1966| N4UAAA|             78.0|          90.0|   62.0|    67.0|    79.0|   DFW| MCI|   460.0|   3.0|   13.0|        0|            null|       0|        67.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|2007|    7|        13|        5| 1840.0|    1745.0| 2007.0|    1905.0|           EV|     4263| N879AS|            147.0|         140.0|  125.0|    62.0|    55.0|   DCA| JAN|   860.0|   6.0|   16.0|        0|            null|       0|        55.0|         0.0|     7.0|          0.0|              0.0|\n",
      "|2006|    2|        26|        7| 1953.0|    1745.0| 2145.0|    1940.0|           OO|     6765| N750SK|            112.0|         115.0|   92.0|   125.0|   128.0|   DEN| TUS|   639.0|   6.0|   14.0|        0|            null|       0|       125.0|         0.0|     0.0|          0.0|              0.0|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air.filter(air.ArrDelay>60).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "air2_pdf = air.select([\"DayOfWeek\", \"ArrDelay\",\"AirTime\",\"Distance\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1009.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DayOfWeek  ArrDelay  AirTime  Distance\n",
       "0        4.0       2.0     25.0     127.0\n",
       "1        7.0      29.0    248.0    1623.0\n",
       "2        NaN       NaN      NaN       NaN\n",
       "3        5.0      -2.0     70.0     451.0\n",
       "4        7.0      11.0    133.0    1009.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air2_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.350591</td>\n",
       "      <td>102.688519</td>\n",
       "      <td>729.997977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArrDelay     AirTime    Distance\n",
       "0  7.350591  102.688519  729.997977"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def myfun(pdf):\n",
    "    out = dict() \n",
    "    out[\"ArrDelay\"] = pdf.ArrDelay.mean()\n",
    "    out[\"AirTime\"]  = pdf.AirTime.mean()\n",
    "    out[\"Distance\"] = pdf.Distance.mean()\n",
    "    \n",
    "    return pd.DataFrame(out, index=[0])\n",
    "\n",
    "myfun(air2_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"DayOfWeek long, ArrDelay long\", PandasUDFType.GROUPED_MAP)   # , AirTime long, Distance long\n",
    "def myfun(pdf):\n",
    "    out = dict() \n",
    "    out[\"ArrDelay\"] = pdf.ArrDelay.mean()\n",
    "#     out[\"AirTime\"]  = pdf.AirTime.mean()\n",
    "#     out[\"Distance\"] = pdf.Distance.mean()\n",
    "    return pd.Dataframe(out, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "air2 = air.select([\"DayOfWeek\",\"ArrDelay\",\"AirTime\",\"Distance\"])# .filter(air.Distance<100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+--------+\n",
      "|DayOfWeek|ArrDelay|AirTime|Distance|\n",
      "+---------+--------+-------+--------+\n",
      "|        4|     2.0|   25.0|   127.0|\n",
      "|        7|    29.0|  248.0|  1623.0|\n",
      "|     null|    null|   null|    null|\n",
      "|        5|    -2.0|   70.0|   451.0|\n",
      "|        7|    11.0|  133.0|  1009.0|\n",
      "|        7|    13.0|  177.0|  1562.0|\n",
      "|        1|   -12.0|  181.0|  1589.0|\n",
      "|        3|    11.0|  364.0|  2611.0|\n",
      "|        5|    13.0|   53.0|   304.0|\n",
      "|     null|    null|   null|    null|\n",
      "|        5|    -8.0|  293.0|  2537.0|\n",
      "|     null|    null|   null|    null|\n",
      "|     null|    null|   null|    null|\n",
      "|        2|    55.0|  285.0|  1927.0|\n",
      "|        1|    23.0|  149.0|   991.0|\n",
      "|        4|    64.0|   35.0|   193.0|\n",
      "|        4|    29.0|   25.0|    77.0|\n",
      "|     null|    null|   null|    null|\n",
      "|        7|    -6.0|   91.0|   678.0|\n",
      "|        7|    35.0|  127.0|   998.0|\n",
      "+---------+--------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "air2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 19:55:07 WARN TaskSetManager: Lost task 24.0 in stage 28.0 (TID 478, emr-worker-2.cluster-46968, executor 2): java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "[Stage 28:>                                                       (0 + 2) / 100]21/12/01 19:55:09 ERROR TaskSetManager: Task 24 in stage 28.0 failed 4 times; aborting job\n",
      "21/12/01 19:55:09 WARN TaskSetManager: Lost task 18.3 in stage 28.0 (TID 487, emr-worker-2.cluster-46968, executor 2): TaskKilled (Stage cancelled)\n",
      "21/12/01 19:55:09 WARN TaskSetManager: Lost task 26.3 in stage 28.0 (TID 488, emr-worker-1.cluster-46968, executor 1): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o241.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 28.0 failed 4 times, most recent failure: Lost task 24.3 in stage 28.0 (TID 486, emr-worker-1.cluster-46968, executor 1): java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16897/2619306587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mair3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mair2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mair3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DayOfWeek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ArrDelay\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DayOfWeek\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark-current/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-current/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-current/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-current/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o241.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 28.0 failed 4 times, most recent failure: Lost task 24.3 in stage 28.0 (TID 486, emr-worker-1.cluster-46968, executor 1): java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "air3 = air2.na.drop()\n",
    "air3.select([\"DayOfWeek\", \"ArrDelay\"]).groupby(\"DayOfWeek\").apply(myfun).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/14 13:43:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/11/14 13:43:23 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/14 13:43:23 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Ltz's Homework\").master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://emr-header-1.cluster-46968:4054\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ltz's Homework</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd3ab9f8d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema_sdf = StructType([\n",
    "        StructField('Year', IntegerType(), True),\n",
    "        StructField('Month', IntegerType(), True),\n",
    "        StructField('DayofMonth', IntegerType(), True),\n",
    "        StructField('DayOfWeek', IntegerType(), True),\n",
    "        StructField('DepTime', DoubleType(), True),\n",
    "        StructField('CRSDepTime', DoubleType(), True),\n",
    "        StructField('ArrTime', DoubleType(), True),\n",
    "        StructField('CRSArrTime', DoubleType(), True),\n",
    "        StructField('UniqueCarrier', StringType(), True),\n",
    "        StructField('FlightNum', StringType(), True),\n",
    "        StructField('TailNum', StringType(), True),\n",
    "        StructField('ActualElapsedTime', DoubleType(), True),\n",
    "        StructField('CRSElapsedTime',  DoubleType(), True),\n",
    "        StructField('AirTime',  DoubleType(), True),\n",
    "        StructField('ArrDelay',  DoubleType(), True),\n",
    "        StructField('DepDelay',  DoubleType(), True),\n",
    "        StructField('Origin', StringType(), True),\n",
    "        StructField('Dest',  StringType(), True),\n",
    "        StructField('Distance',  DoubleType(), True),\n",
    "        StructField('TaxiIn',  DoubleType(), True),\n",
    "        StructField('TaxiOut',  DoubleType(), True),\n",
    "        StructField('Cancelled',  IntegerType(), True),\n",
    "        StructField('CancellationCode',  StringType(), True),\n",
    "        StructField('Diverted',  IntegerType(), True),\n",
    "        StructField('CarrierDelay', DoubleType(), True),\n",
    "        StructField('WeatherDelay',  DoubleType(), True),\n",
    "        StructField('NASDelay',  DoubleType(), True),\n",
    "        StructField('SecurityDelay',  DoubleType(), True),\n",
    "        StructField('LateAircraftDelay',  DoubleType(), True)\n",
    "    ])\n",
    "air = spark.read.options(header='true').schema(schema_sdf).csv(\"/lifeng/student/liutuozhen/airdelay_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================>                       (3 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          ArrDelay|\n",
      "+-------+------------------+\n",
      "|  count|           5432958|\n",
      "|   mean|  6.97897995898367|\n",
      "| stddev|30.191156753519532|\n",
      "|    min|           -1238.0|\n",
      "|    max|            1779.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "air.describe(['ArrDelay']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 19:04:19 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Year=2006, Month=7, DayofMonth=6, DayOfWeek=4, DepTime=2055.0, CRSDepTime=2055.0, ArrTime=2150.0, CRSArrTime=2148.0, UniqueCarrier='XE', FlightNum='2619', TailNum='N11526', ActualElapsedTime=55.0, CRSElapsedTime=53.0, AirTime=25.0, ArrDelay=2.0, DepDelay=0.0, Origin='IAH', Dest='LCH', Distance=127.0, TaxiIn=8.0, TaxiOut=22.0, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=0.0, WeatherDelay=0.0, NASDelay=0.0, SecurityDelay=0.0, LateAircraftDelay=0.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "        inputCols=[\"DepDelay\", \"Distance\"],\n",
    "        outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = assembler.transform(air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: double, CRSDepTime: double, ArrTime: double, CRSArrTime: double, UniqueCarrier: string, FlightNum: string, TailNum: string, ActualElapsedTime: double, CRSElapsedTime: double, AirTime: double, ArrDelay: double, DepDelay: double, Origin: string, Dest: string, Distance: double, TaxiIn: double, TaxiOut: double, Cancelled: int, CancellationCode: string, Diverted: int, CarrierDelay: double, WeatherDelay: double, NASDelay: double, SecurityDelay: double, LateAircraftDelay: double, features: vector]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_1edf0b17acf2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+\n",
      "|label|            sentence|               words|        rawFeatures|\n",
      "+-----+--------------------+--------------------+-------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(2,[0,1],[1.0,4.0])|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(2,[0,1],[1.0,6.0])|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(2,[0,1],[3.0,2.0])|\n",
      "+-----+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[0,5,9,17],[0...|\n",
      "|  0.0|(20,[2,7,9,13,15]...|\n",
      "|  1.0|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema_sdf = StructType([\n",
    "        StructField('date', StringType(), True),\n",
    "        StructField('rating', IntegerType(), True),\n",
    "        StructField('review', StringType(), True)\n",
    "    ])\n",
    "review = spark.read.options(header='true').schema(schema_sdf).csv(\"file:///home/student/student/liutuozhen/spark/avengers_review.csv\")\n",
    "review = review.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+\n",
      "|     date|rating|              review|\n",
      "+---------+------+--------------------+\n",
      "|29-Apr-19|    10|I only have One w...|\n",
      "|29-Apr-19|     9|A very decent end...|\n",
      "|18-Sep-19|    10|\"I have to say In...|\n",
      "|28-Apr-19|    10|Man this film goe...|\n",
      "|27-Apr-19|    10|More than 10 year...|\n",
      "|22-Jun-19|    10|Just magnificentl...|\n",
      "|10-Jul-19|    10|Avengers: Endgame...|\n",
      "|27-May-19|    10|Reading through s...|\n",
      "|27-Apr-19|    10|\"This movie was t...|\n",
      "|28-Dec-19|     4|\"Infinity War was...|\n",
      "|25-Apr-19|    10|This is what all ...|\n",
      "|14-Apr-20|     1|If one Avengers p...|\n",
      "|23-Jun-19|    10|This movie was so...|\n",
      "|30-Apr-19|     5|A messed up past,...|\n",
      "|28-Apr-19|     4|Infinity wars fel...|\n",
      "| 8-May-19|     1|Far from what one...|\n",
      "| 5-May-19|     9|\"There is a weird...|\n",
      "|26-Apr-19|    10|No words. What an...|\n",
      "|28-Apr-19|    10|Loved the jokes. ...|\n",
      "|27-Apr-19|    10|Simply amazing fr...|\n",
      "+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_f83adb6ef6dd"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_words = tokenizer.transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+--------------------+\n",
      "|     date|rating|              review|               words|\n",
      "+---------+------+--------------------+--------------------+\n",
      "|29-Apr-19|    10|I only have One w...|[i, only, have, o...|\n",
      "|29-Apr-19|     9|A very decent end...|[a, very, decent,...|\n",
      "|18-Sep-19|    10|\"I have to say In...|[\"i, have, to, sa...|\n",
      "|28-Apr-19|    10|Man this film goe...|[man, this, film,...|\n",
      "|27-Apr-19|    10|More than 10 year...|[more, than, 10, ...|\n",
      "|22-Jun-19|    10|Just magnificentl...|[just, magnificen...|\n",
      "|10-Jul-19|    10|Avengers: Endgame...|[avengers:, endga...|\n",
      "|27-May-19|    10|Reading through s...|[reading, through...|\n",
      "|27-Apr-19|    10|\"This movie was t...|[\"this, movie, wa...|\n",
      "|28-Dec-19|     4|\"Infinity War was...|[\"infinity, war, ...|\n",
      "|25-Apr-19|    10|This is what all ...|[this, is, what, ...|\n",
      "|14-Apr-20|     1|If one Avengers p...|[if, one, avenger...|\n",
      "|23-Jun-19|    10|This movie was so...|[this, movie, was...|\n",
      "|30-Apr-19|     5|A messed up past,...|[a, messed, up, p...|\n",
      "|28-Apr-19|     4|Infinity wars fel...|[infinity, wars, ...|\n",
      "| 8-May-19|     1|Far from what one...|[far, from, what,...|\n",
      "| 5-May-19|     9|\"There is a weird...|[\"there, is, a, w...|\n",
      "|26-Apr-19|    10|No words. What an...|[no, words., what...|\n",
      "|28-Apr-19|    10|Loved the jokes. ...|[loved, the, joke...|\n",
      "|27-Apr-19|    10|Simply amazing fr...|[simply, amazing,...|\n",
      "+---------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|            features|\n",
      "+------+--------------------+\n",
      "|    10|(100,[3,4,5,10,13...|\n",
      "|     9|(100,[6,10,18,23,...|\n",
      "|    10|(100,[2,5,10,11,1...|\n",
      "|    10|(100,[3,8,10,24,3...|\n",
      "|    10|(100,[0,4,5,7,8,1...|\n",
      "|    10|(100,[3,5,7,10,16...|\n",
      "|    10|(100,[70,80,82],[...|\n",
      "|    10|(100,[0,1,3,5,8,1...|\n",
      "|    10|(100,[0,2,3,4,5,9...|\n",
      "|     4|(100,[0,1,2,4,5,6...|\n",
      "|    10|(100,[12,14,16,19...|\n",
      "|     1|(100,[1,2,3,6,7,1...|\n",
      "|    10|(100,[5,7,10,13,1...|\n",
      "|     5|(100,[3,7,10,11,1...|\n",
      "|     4|(100,[4,7,10,15,2...|\n",
      "|     1|(100,[0,3,5,6,7,1...|\n",
      "|     9|(100,[0,3,5,6,7,1...|\n",
      "|    10|(100,[2,5,10,15,1...|\n",
      "|    10|(100,[4,6,10,15,1...|\n",
      "|    10|(100,[0,1,2,3,4,8...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(review_words)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "data = rescaledData.select(\"rating\", \"features\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(k=2, maxIter=10)\n",
    "model = lda.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "|topic|termIndices |termWeights                                                       |\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "|0    |[25, 81, 63]|[0.02267453318781094, 0.018067881190574377, 0.016521117328933677] |\n",
      "|1    |[29, 34, 10]|[0.017304295666922084, 0.017041683961121077, 0.016887362881921438]|\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------+\n",
      "|rating|topicDistribution                       |\n",
      "+------+----------------------------------------+\n",
      "|10    |[0.21331872856513448,0.7866812714348655]|\n",
      "|9     |[0.858526604447247,0.14147339555275304] |\n",
      "|10    |[0.9016935807101404,0.09830641928985952]|\n",
      "|10    |[0.05039997211340099,0.949600027886599] |\n",
      "|10    |[0.44083058592262064,0.5591694140773794]|\n",
      "|10    |[0.7898507840379052,0.21014921596209485]|\n",
      "|10    |[0.21348695456463335,0.7865130454353666]|\n",
      "|10    |[0.6167794721858191,0.383220527814181]  |\n",
      "|10    |[0.0232296312137377,0.9767703687862622] |\n",
      "|4     |[0.38852605722918426,0.6114739427708157]|\n",
      "|10    |[0.13327275349533668,0.8667272465046633]|\n",
      "|1     |[0.7641206540065714,0.23587934599342858]|\n",
      "|10    |[0.3957236585774921,0.6042763414225079] |\n",
      "|5     |[0.18754884879382588,0.8124511512061741]|\n",
      "|4     |[0.051068691642869986,0.94893130835713] |\n",
      "|1     |[0.7205224590943771,0.27947754090562305]|\n",
      "|9     |[0.29865243586235646,0.7013475641376435]|\n",
      "|10    |[0.16368643562463409,0.836313564375366] |\n",
      "|10    |[0.31141499347922996,0.68858500652077]  |\n",
      "|10    |[0.5282397850895073,0.47176021491049275]|\n",
      "+------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(data)\n",
    "transformed.select(\"rating\", \"topicDistribution\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "vectorToColumn = fn.udf(lambda vec: vec[0].item(), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_probs = transformed.withColumn(\"topic_prob\", vectorToColumn(transformed[\"topicDistribution\"])).select(\"rating\", \"topic_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|          topic_prob|\n",
      "+------+--------------------+\n",
      "|    10| 0.21331872856513448|\n",
      "|     9|   0.858526604447247|\n",
      "|    10|  0.9016935807101404|\n",
      "|    10| 0.05039997211340099|\n",
      "|    10| 0.44083058592262064|\n",
      "|    10|  0.7898507840379052|\n",
      "|    10| 0.21348695456463335|\n",
      "|    10|  0.6167794721858191|\n",
      "|    10|  0.0232296312137377|\n",
      "|     4| 0.38852605722918426|\n",
      "|    10| 0.13327275349533668|\n",
      "|     1|  0.7641206540065714|\n",
      "|    10|  0.3957236585774921|\n",
      "|     5| 0.18754884879382588|\n",
      "|     4|0.051068691642869986|\n",
      "|     1|  0.7205224590943771|\n",
      "|     9| 0.29865243586235646|\n",
      "|    10| 0.16368643562463409|\n",
      "|    10| 0.31141499347922996|\n",
      "|    10|  0.5282397850895073|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_probs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 170, in manager\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 73, in worker\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/worker.py\", line 402, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 170, in manager\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 73, in worker\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/worker.py\", line 402, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 114:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10818118],\n",
      "             [0.10818118, 1.        ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"rating\", \"topic_prob\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_for_cor = assembler.transform(transformed_probs)\n",
    "r1 = Correlation.corr(df_for_cor, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

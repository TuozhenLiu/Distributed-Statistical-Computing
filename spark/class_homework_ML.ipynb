{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No such property [datePattern] in org.apache.log4j.RollingFileAppender.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "21/12/18 14:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Ltz's Homework\").master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://emr-header-1.cluster-46968:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ltz's Homework</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb22b157d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema_sdf = StructType([\n",
    "        StructField('Usage', DoubleType(), True),\n",
    "        StructField('Limit', DoubleType(), True),\n",
    "        StructField('MortgageStatus', IntegerType(), True),\n",
    "        StructField('HistoryStatus', IntegerType(), True),\n",
    "        StructField('CountStatus', IntegerType(), True),\n",
    "        StructField('OverdueStatus', IntegerType(), True),\n",
    "        StructField('Gender_male', IntegerType(), True)\n",
    "    ])\n",
    "credit = spark.read.options(header='true').schema(schema_sdf).csv(\"file:///home/student/student/liutuozhen/spark/credit_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------+-------------+-----------+-------------+-----------+\n",
      "|Usage| Limit|MortgageStatus|HistoryStatus|CountStatus|OverdueStatus|Gender_male|\n",
      "+-----+------+--------------+-------------+-----------+-------------+-----------+\n",
      "| 1.01|1000.0|             0|            0|          1|            1|          1|\n",
      "|  1.0|1000.0|             0|            1|          1|            1|          1|\n",
      "| 1.01|1000.0|             0|            1|          1|            1|          1|\n",
      "|  1.0|1000.0|             0|            1|          1|            1|          1|\n",
      "| 0.98|1000.0|             0|            1|          1|            1|          1|\n",
      "|  1.0|1000.0|             0|            1|          1|            1|          1|\n",
      "|  1.0|1000.0|             0|            1|          1|            1|          1|\n",
      "| 0.95|1000.0|             0|            0|          1|            1|          0|\n",
      "| 0.35|1000.0|             0|            1|          1|            1|          0|\n",
      "| 0.43|1000.0|             0|            0|          1|            1|          1|\n",
      "|  0.2|1000.0|             0|            0|          1|            0|          1|\n",
      "| 0.59|1000.0|             0|            0|          1|            1|          0|\n",
      "| 0.99|1000.0|             0|            1|          1|            1|          1|\n",
      "| 0.99|1000.0|             1|            1|          1|            1|          1|\n",
      "|  1.0|1000.0|             0|            0|          1|            1|          0|\n",
      "| 0.99|1000.0|             0|            0|          1|            0|          1|\n",
      "|  0.0|1000.0|             0|            0|          0|            0|          1|\n",
      "|  0.4|1000.0|             0|            1|          1|            1|          1|\n",
      "| 0.19|1000.0|             0|            0|          1|            1|          1|\n",
      "|  1.0|1000.0|             1|            1|          1|            0|          0|\n",
      "+-----+------+--------------+-------------+-----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|     OverdueStatus|\n",
      "+-------+------------------+\n",
      "|  count|              8371|\n",
      "|   mean|0.6155775892963804|\n",
      "| stddev|0.4864875059766857|\n",
      "|    min|                 0|\n",
      "|    max|                 1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit.describe(['OverdueStatus']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "        inputCols=[\"Usage\", \"Limit\", \"MortgageStatus\", \"HistoryStatus\", \"CountStatus\", \"Gender_male\"],\n",
    "        outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = assembler.transform(credit).select(\"OverdueStatus\", \"features\").withColumnRenamed(\"OverdueStatus\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (20% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "def predict(name, model, testData):\n",
    "    predictions = model.transform(testData)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"{name} | Test Accuracy = {round(accuracy, 6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression | Test Accuracy = 0.706201\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=100, regParam=0.01)\n",
    "model = lr.fit(trainingData)\n",
    "predict(\"LogisticRegression\", model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier | Test Accuracy = 0.710415\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=3)\n",
    "model = dt.fit(trainingData)\n",
    "predict(\"DecisionTreeClassifier\", model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier | Test Accuracy = 0.71162\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(numTrees=10, maxDepth=5)\n",
    "model = rf.fit(trainingData)\n",
    "predict(\"RandomForestClassifier\", model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassifier | Test Accuracy = 0.71162\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(maxIter=10)\n",
    "model = gbt.fit(trainingData)\n",
    "predict(\"GBTClassifier\", model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC | Test Accuracy = 0.692354\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.01)\n",
    "model = lsvc.fit(trainingData)\n",
    "predict(\"LinearSVC\", model, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema_sdf = StructType([\n",
    "        StructField('date', StringType(), True),\n",
    "        StructField('rating', IntegerType(), True),\n",
    "        StructField('review', StringType(), True)\n",
    "    ])\n",
    "review = spark.read.options(header='true').schema(schema_sdf).csv(\"file:///home/student/student/liutuozhen/spark/avengers_review.csv\")\n",
    "review = review.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+\n",
      "|     date|rating|              review|\n",
      "+---------+------+--------------------+\n",
      "|29-Apr-19|    10|I only have One w...|\n",
      "|29-Apr-19|     9|A very decent end...|\n",
      "|18-Sep-19|    10|\"I have to say In...|\n",
      "|28-Apr-19|    10|Man this film goe...|\n",
      "|27-Apr-19|    10|More than 10 year...|\n",
      "|22-Jun-19|    10|Just magnificentl...|\n",
      "|10-Jul-19|    10|Avengers: Endgame...|\n",
      "|27-May-19|    10|Reading through s...|\n",
      "|27-Apr-19|    10|\"This movie was t...|\n",
      "|28-Dec-19|     4|\"Infinity War was...|\n",
      "|25-Apr-19|    10|This is what all ...|\n",
      "|14-Apr-20|     1|If one Avengers p...|\n",
      "|23-Jun-19|    10|This movie was so...|\n",
      "|30-Apr-19|     5|A messed up past,...|\n",
      "|28-Apr-19|     4|Infinity wars fel...|\n",
      "| 8-May-19|     1|Far from what one...|\n",
      "| 5-May-19|     9|\"There is a weird...|\n",
      "|26-Apr-19|    10|No words. What an...|\n",
      "|28-Apr-19|    10|Loved the jokes. ...|\n",
      "|27-Apr-19|    10|Simply amazing fr...|\n",
      "+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_f83adb6ef6dd"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_words = tokenizer.transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+--------------------+\n",
      "|     date|rating|              review|               words|\n",
      "+---------+------+--------------------+--------------------+\n",
      "|29-Apr-19|    10|I only have One w...|[i, only, have, o...|\n",
      "|29-Apr-19|     9|A very decent end...|[a, very, decent,...|\n",
      "|18-Sep-19|    10|\"I have to say In...|[\"i, have, to, sa...|\n",
      "|28-Apr-19|    10|Man this film goe...|[man, this, film,...|\n",
      "|27-Apr-19|    10|More than 10 year...|[more, than, 10, ...|\n",
      "|22-Jun-19|    10|Just magnificentl...|[just, magnificen...|\n",
      "|10-Jul-19|    10|Avengers: Endgame...|[avengers:, endga...|\n",
      "|27-May-19|    10|Reading through s...|[reading, through...|\n",
      "|27-Apr-19|    10|\"This movie was t...|[\"this, movie, wa...|\n",
      "|28-Dec-19|     4|\"Infinity War was...|[\"infinity, war, ...|\n",
      "|25-Apr-19|    10|This is what all ...|[this, is, what, ...|\n",
      "|14-Apr-20|     1|If one Avengers p...|[if, one, avenger...|\n",
      "|23-Jun-19|    10|This movie was so...|[this, movie, was...|\n",
      "|30-Apr-19|     5|A messed up past,...|[a, messed, up, p...|\n",
      "|28-Apr-19|     4|Infinity wars fel...|[infinity, wars, ...|\n",
      "| 8-May-19|     1|Far from what one...|[far, from, what,...|\n",
      "| 5-May-19|     9|\"There is a weird...|[\"there, is, a, w...|\n",
      "|26-Apr-19|    10|No words. What an...|[no, words., what...|\n",
      "|28-Apr-19|    10|Loved the jokes. ...|[loved, the, joke...|\n",
      "|27-Apr-19|    10|Simply amazing fr...|[simply, amazing,...|\n",
      "+---------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|            features|\n",
      "+------+--------------------+\n",
      "|    10|(100,[3,4,5,10,13...|\n",
      "|     9|(100,[6,10,18,23,...|\n",
      "|    10|(100,[2,5,10,11,1...|\n",
      "|    10|(100,[3,8,10,24,3...|\n",
      "|    10|(100,[0,4,5,7,8,1...|\n",
      "|    10|(100,[3,5,7,10,16...|\n",
      "|    10|(100,[70,80,82],[...|\n",
      "|    10|(100,[0,1,3,5,8,1...|\n",
      "|    10|(100,[0,2,3,4,5,9...|\n",
      "|     4|(100,[0,1,2,4,5,6...|\n",
      "|    10|(100,[12,14,16,19...|\n",
      "|     1|(100,[1,2,3,6,7,1...|\n",
      "|    10|(100,[5,7,10,13,1...|\n",
      "|     5|(100,[3,7,10,11,1...|\n",
      "|     4|(100,[4,7,10,15,2...|\n",
      "|     1|(100,[0,3,5,6,7,1...|\n",
      "|     9|(100,[0,3,5,6,7,1...|\n",
      "|    10|(100,[2,5,10,15,1...|\n",
      "|    10|(100,[4,6,10,15,1...|\n",
      "|    10|(100,[0,1,2,3,4,8...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(review_words)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "data = rescaledData.select(\"rating\", \"features\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(k=2, maxIter=10)\n",
    "model = lda.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "|topic|termIndices |termWeights                                                       |\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "|0    |[25, 81, 63]|[0.02267453318781094, 0.018067881190574377, 0.016521117328933677] |\n",
      "|1    |[29, 34, 10]|[0.017304295666922084, 0.017041683961121077, 0.016887362881921438]|\n",
      "+-----+------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------+\n",
      "|rating|topicDistribution                       |\n",
      "+------+----------------------------------------+\n",
      "|10    |[0.21331872856513448,0.7866812714348655]|\n",
      "|9     |[0.858526604447247,0.14147339555275304] |\n",
      "|10    |[0.9016935807101404,0.09830641928985952]|\n",
      "|10    |[0.05039997211340099,0.949600027886599] |\n",
      "|10    |[0.44083058592262064,0.5591694140773794]|\n",
      "|10    |[0.7898507840379052,0.21014921596209485]|\n",
      "|10    |[0.21348695456463335,0.7865130454353666]|\n",
      "|10    |[0.6167794721858191,0.383220527814181]  |\n",
      "|10    |[0.0232296312137377,0.9767703687862622] |\n",
      "|4     |[0.38852605722918426,0.6114739427708157]|\n",
      "|10    |[0.13327275349533668,0.8667272465046633]|\n",
      "|1     |[0.7641206540065714,0.23587934599342858]|\n",
      "|10    |[0.3957236585774921,0.6042763414225079] |\n",
      "|5     |[0.18754884879382588,0.8124511512061741]|\n",
      "|4     |[0.051068691642869986,0.94893130835713] |\n",
      "|1     |[0.7205224590943771,0.27947754090562305]|\n",
      "|9     |[0.29865243586235646,0.7013475641376435]|\n",
      "|10    |[0.16368643562463409,0.836313564375366] |\n",
      "|10    |[0.31141499347922996,0.68858500652077]  |\n",
      "|10    |[0.5282397850895073,0.47176021491049275]|\n",
      "+------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(data)\n",
    "transformed.select(\"rating\", \"topicDistribution\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "vectorToColumn = fn.udf(lambda vec: vec[0].item(), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_probs = transformed.withColumn(\"topic_prob\", vectorToColumn(transformed[\"topicDistribution\"])).select(\"rating\", \"topic_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|rating|          topic_prob|\n",
      "+------+--------------------+\n",
      "|    10| 0.21331872856513448|\n",
      "|     9|   0.858526604447247|\n",
      "|    10|  0.9016935807101404|\n",
      "|    10| 0.05039997211340099|\n",
      "|    10| 0.44083058592262064|\n",
      "|    10|  0.7898507840379052|\n",
      "|    10| 0.21348695456463335|\n",
      "|    10|  0.6167794721858191|\n",
      "|    10|  0.0232296312137377|\n",
      "|     4| 0.38852605722918426|\n",
      "|    10| 0.13327275349533668|\n",
      "|     1|  0.7641206540065714|\n",
      "|    10|  0.3957236585774921|\n",
      "|     5| 0.18754884879382588|\n",
      "|     4|0.051068691642869986|\n",
      "|     1|  0.7205224590943771|\n",
      "|     9| 0.29865243586235646|\n",
      "|    10| 0.16368643562463409|\n",
      "|    10| 0.31141499347922996|\n",
      "|    10|  0.5282397850895073|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_probs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 170, in manager\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 73, in worker\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/worker.py\", line 402, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 170, in manager\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/daemon.py\", line 73, in worker\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/worker.py\", line 402, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/lib/spark-current/python/lib/pyspark.zip/pyspark/serializers.py\", line 724, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 114:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10818118],\n",
      "             [0.10818118, 1.        ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"rating\", \"topic_prob\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_for_cor = assembler.transform(transformed_probs)\n",
    "r1 = Correlation.corr(df_for_cor, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version 2.11.12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.Properties.versionString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## breeze库 类似numpy Scala的科学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import breeze.linalg._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x = DenseVector(0.0, 0.0, 0.0, 0.0, 0.0)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DenseVector(0.0, 0.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = DenseVector.zeros[Double](5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object scala.Double"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Double // Double是scala的一个对象， DenseVector是linalg库里的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark的ml库(基于sdf，mllib基于rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@4142aacc"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "linesWithSpark = [value: boolean]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: boolean]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val linesWithSpark = textFile.filter(line => line.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a = Array(1, 2, 3, 4, 5)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a: Array[Int] =Array(1,2,3,4,5)\n",
    "//直接遍历输出\n",
    "    for (i <- a ) {\n",
    "        println(i)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myFunc: (s: String)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s: String): Int = {\n",
    "    val s_array =  s.split(\",\")\n",
    "    var len = 0\n",
    "    for( x <- s_array ){\n",
    "        if( x != \"北京\" ) len += 1\n",
    "    }\n",
    "    return len\n",
    "}\n",
    "textFile.map(myFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = Array(1, 2, 3, 4, 5)\n",
       "counter = 0\n",
       "rdd = ParallelCollectionRDD[32] at parallelize at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[32] at parallelize at <console>:39"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "var counter = 0\n",
    "var rdd = sc.parallelize(data)\n",
    "\n",
    "// Wrong: Don't do this!!\n",
    "rdd.foreach(x => counter += x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkContext, SparkConf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@4142aacc"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc = org.apache.spark.SparkContext@cbc8eef\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@cbc8eef"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@cbc8eef"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = Array(1, 2, 3, 4, 5)\n",
       "distData = ParallelCollectionRDD[33] at parallelize at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[33] at parallelize at <console>:35"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:39: error: value toDF is not a member of org.apache.spark.rdd.RDD[org.apache.spark.sql.Row]\n",
       "possible cause: maybe a semicolon is missing before `value toDF'?\n",
       "         .toDF()\n",
       "          ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// For implicit conversions from RDDs to DataFrames\n",
    "// import spark.implicits._\n",
    "\n",
    "// Create an RDD from a text file, convert it to a Dataframe\n",
    "val spark_df = sc\n",
    "  .textFile(\"/lifeng/student/liutuozhen/streaming/aggregate/table.csv\")\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => (attributes(0), attributes(2).toInt))\n",
    "  .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "|北京|  7|\n",
      "|江苏|  8|\n",
      "|上海| 18|\n",
      "|北京| 45|\n",
      "|江苏| 77|\n",
      "|江苏| 24|\n",
      "|江苏| 77|\n",
      "|江苏| 31|\n",
      "|上海| 71|\n",
      "|江苏| 57|\n",
      "|上海| 72|\n",
      "|浙江| 66|\n",
      "|北京| 19|\n",
      "|江苏|  9|\n",
      "|浙江| 72|\n",
      "|北京| 16|\n",
      "|北京| 61|\n",
      "|北京| 21|\n",
      "|浙江| 29|\n",
      "|上海| 34|\n",
      "+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.createTempView(\"tempDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "|北京|  7|\n",
      "|江苏|  8|\n",
      "|上海| 18|\n",
      "|北京| 45|\n",
      "|江苏| 77|\n",
      "|江苏| 24|\n",
      "|江苏| 77|\n",
      "|江苏| 31|\n",
      "|上海| 71|\n",
      "|江苏| 57|\n",
      "|上海| 72|\n",
      "|浙江| 66|\n",
      "|北京| 19|\n",
      "|江苏|  9|\n",
      "|浙江| 72|\n",
      "|北京| 16|\n",
      "|北京| 61|\n",
      "|北京| 21|\n",
      "|浙江| 29|\n",
      "|上海| 34|\n",
      "+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM tempDF\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array({\"_1\":\"北京\",\"_2\":7}, {\"_1\":\"江苏\",\"_2\":8}, {\"_1\":\"上海\",\"_2\":18}, {\"_1\":\"北京\",\"_2\":45}, {\"_1\":\"江苏\",\"_2\":77}, {\"_1\":\"江苏\",\"_2\":24}, {\"_1\":\"江苏\",\"_2\":77}, {\"_1\":\"江苏\",\"_2\":31}, {\"_1\":\"上海\",\"_2\":71}, {\"_1\":\"江苏\",\"_2\":57}, {\"_1\":\"上海\",\"_2\":72}, {\"_1\":\"浙江\",\"_2\":66}, {\"_1\":\"北京\",\"_2\":19}, {\"_1\":\"江苏\",\"_2\":9}, {\"_1\":\"浙江\",\"_2\":72}, {\"_1\":\"北京\",\"_2\":16}, {\"_1\":\"北京\",\"_2\":61}, {\"_1\":\"北京\",\"_2\":21}, {\"_1\":\"浙江\",\"_2\":29}, {\"_1\":\"上海\",\"_2\":34}, {\"_1\":\"浙江\",\"_2\":26}, {\"_1\":\"江苏\",\"_2\":30}, {\"_1\":\"江苏\",\"_2\":65}, {\"_1\":\"浙江\",\"_2\":66}, {\"_1\":\"北京\",\"_2\":38}, {\"_1\":\"上海\",\"_2\":37}, {\"_1\":\"上海\",\"_2\":75}, {\"_1\":\"上海\",\"_2\":36}, {\"_1\":\"上海\",\"_2\":18}, {\"_1\":\"北京\",\"_2\":76}, {\"_1\":\"上海\",\"_2\":61}, {\"_1\":\"北京\",\"_2\":29}, {\"_1\":\"江苏\",\"_2\":46}, {\"_1\":\"北京\",\"_2\":23}, {\"_1\":\"江苏\",\"_2\":45}, {\"_1\":\"上海\",\"_2\":59}, {\"_1\":\"浙江\",\"_2\"..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.toJSON.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
